{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0528c6d1",
   "metadata": {},
   "source": [
    "# **Fake Reviews - Modeling**\n",
    "---\n",
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa003197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score, auc, roc_auc_score, roc_curve, cohen_kappa_score\n",
    "\n",
    "#algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, SelectPercentile\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# set this to True to run hyperparameter tuning for each model\n",
    "run_hyperparameter_tuning=False\n",
    "\n",
    "# DataFrame to store the results of each model\n",
    "results = pd.DataFrame(columns=['accuracy', 'balanced-accuracy', 'roc-auc', 'precision', 'recall', 'f1', 'kappa', 'time', 'model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format the confusion matrix\n",
    "def format_confusion_matrix(conf):\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf.flatten()/np.sum(conf)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names, group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    return labels\n",
    "\n",
    "# function to create a confusion matrix heatmap\n",
    "def create_conf_heatmap(conf, acc, cross, report, auc, kappa, title):\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    ax = sns.heatmap(conf, annot=format_confusion_matrix(conf), fmt='', cmap='Blues')\n",
    "    ax.set_title('{0} Confusion Matrix\\n\\naccuracy: {1:0.4f};\\ncross-validation (k=10): {2:0.4f};\\nauc: {3:0.4f};\\nkappa: {4:0.4f}' \n",
    "                 .format(title, acc, cross, auc, kappa), fontsize=10)\n",
    "    ax.set_xlabel('\\nPredicted Values\\n\\nMetrics:\\nprecision    recall    f1-score\\n0 - {:0.4f}   {:0.4f}   {:0.4f}\\n1 - {:0.4f}   {:0.4f}   {:0.4f}\\navg - {:0.4f}   {:0.4f}   {:0.4f}'.format(\n",
    "        report[\"0\"][\"precision\"], report[\"0\"][\"recall\"], report[\"0\"][\"f1-score\"],                                                                                                      \n",
    "        report[\"1\"][\"precision\"], report[\"1\"][\"recall\"], report[\"1\"][\"f1-score\"],\n",
    "        report[\"weighted avg\"][\"precision\"], report[\"weighted avg\"][\"recall\"], report[\"weighted avg\"][\"f1-score\"],))\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.xaxis.set_ticklabels(['Deceptive','Truthful'])\n",
    "    ax.yaxis.set_ticklabels(['Deceptive','Truthful'])\n",
    "    return ax\n",
    "\n",
    "# function to train and evaluate a model\n",
    "def train_model(base_model, model_name):\n",
    "    start_time = time.perf_counter()\n",
    "    model = base_model.fit(X_train, y_train)\n",
    "    model_time = time.perf_counter() - start_time\n",
    "\n",
    "    predict = model.predict(X_test)\n",
    "    conf = confusion_matrix(y_test, predict)\n",
    "    acc = accuracy_score(y_test, predict)\n",
    "    bacc = balanced_accuracy_score(y_test, predict)\n",
    "    kappa = cohen_kappa_score(y_test, predict)\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    cross = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1).mean()\n",
    "    report = classification_report(y_test, predict, output_dict=True)\n",
    "\n",
    "    #ROC - AUC\n",
    "    try:\n",
    "        pred_prob = model.predict_proba(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, pred_prob[:,1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    except:\n",
    "        roc_auc = 0\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    ax = create_conf_heatmap(conf, acc, cross, report, roc_auc, kappa, title = model_name)\n",
    "    plt.show()\n",
    "    \n",
    "    new_row = [acc, bacc, roc_auc, report[\"0\"][\"precision\"], report[\"0\"][\"recall\"], report[\"0\"][\"f1-score\"], kappa, model_time, base_model]\n",
    "    new_row[:2] = [round(value * 100, 2) for value in new_row[:2]]\n",
    "    new_row[3:6] = [round(value * 100, 2) for value in new_row[3:6]]\n",
    "    new_row[:8] = [round(value, 2) for value in new_row[:8]]\n",
    "    results.loc[model_name] = new_row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d3f2f4",
   "metadata": {},
   "source": [
    "---\n",
    "## **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_pickle('t10.pkl')\n",
    "\n",
    "# split the dataset into features and target\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4694875",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d1eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.20565123083486536, solver='liblinear')\n",
    "train_model(lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    param_grid = [    \n",
    "        {#'penalty' : ['l1', 'l2'],\n",
    "        'C': np.logspace(-4, 4, 500),\n",
    "        #'solver' : ['lbfgs','newton-cg','liblinear','sag','saga', 'newton-cholesky'],\n",
    "        'solver' : ['liblinear'],\n",
    "        #'max_iter' : [100, 1000, 2500, 5000, 10000]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, scoring='accuracy', verbose=True, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eec91c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 100, 50),\n",
    "              learning_rate='adaptive', solver='sgd')\n",
    "train_model(mlp, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1152100",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(MLPClassifier(), parameter_space, scoring='accuracy', n_jobs=-1, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameter set\n",
    "    print('Best parameters found:\\n', clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d03954",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1400d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=75, min_samples_leaf=21)\n",
    "train_model(dt, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a32cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    params = {\n",
    "        'max_depth': np.arange(2, 100, 5),\n",
    "        'min_samples_leaf': np.arange(2, 100, 5),\n",
    "        'criterion': [\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(estimator=DecisionTreeClassifier(), \n",
    "                               param_grid=params, \n",
    "                               n_jobs=-1, verbose=1, scoring=\"accuracy\")\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f395510",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08961c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "train_model(nb, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948dc88",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c07196",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=100, degree=2, gamma=0.03, kernel='poly', probability=True)\n",
    "train_model(svm, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvm = LinearSVC(C=0.1)\n",
    "train_model(linsvm, 'Linear SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    # instantiate classifier with default hyperparameters with kernel=rbf, C=1.0 and gamma=auto\n",
    "    svc = SVC(probability=True) \n",
    "\n",
    "    # declare parameters for hyperparameter tuning\n",
    "    parameters = [ #{'C':[1, 10, 100, 1000], 'kernel':['linear']},\n",
    "                   #{'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "                   {'C':[1, 10, 100, 1000], 'kernel':['poly'],\n",
    "                    'degree': [2,3,4,5],\n",
    "                    'gamma':[0.01,0.02,0.03,0.04,0.05,0.06,0.07]} \n",
    "                  ]\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = svc,  \n",
    "                               param_grid = parameters,\n",
    "                               scoring = 'accuracy',\n",
    "                               cv = cv,\n",
    "                               n_jobs = -1,\n",
    "                               verbose=1)\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "    print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57522b",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5349cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(metric='manhattan', n_neighbors=30, weights='distance')\n",
    "train_model(knn, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    grid_params = { 'n_neighbors' : list(range(1, 50, 1)),\n",
    "                    'weights' : ['uniform','distance'],\n",
    "                    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                    'metric' : ['minkowski','euclidean','manhattan']\n",
    "                  }\n",
    "\n",
    "    clf = GridSearchCV(KNeighborsClassifier(), grid_params, verbose=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72386495",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = PassiveAggressiveClassifier(max_iter=1000, random_state=7, tol=1e-3)\n",
    "train_model(pa, 'Passive Aggressive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bccf8",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=12, max_features='log2', n_estimators=300)\n",
    "train_model(rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    grid_params = { 'n_estimators' : [100, 200, 300, 400, 500],\n",
    "                   #'criterion' : ['gini','entropy', 'log_loss'],\n",
    "                   'max_depth': list(range(1, 30, 1)),\n",
    "                   #'max_features' : ['sqrt','log2'],\n",
    "                   #'sampling_strategy' : ['majority','not minority','not majority','all'],\n",
    "                  }\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), grid_params, verbose=1, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3275ea",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=4)\n",
    "train_model(gb, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a58886",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    params = {\n",
    "        \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 250, 500],\n",
    "        \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 10],\n",
    "        \"max_depth\": np.arange(1, 10, 2).tolist(),\n",
    "    }\n",
    "\n",
    "    search_cv = GridSearchCV(GradientBoostingClassifier(), param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "    search_cv.fit(X_train, y_train)\n",
    "    print(search_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f353af5",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02260da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
    "              early_stopping_rounds=None, enable_categorical=False,\n",
    "              eval_metric=None, gamma=1.5, gpu_id=-1, grow_policy='depthwise',\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.5, max_bin=256, max_cat_to_onehot=4,\n",
    "              max_delta_step=0, max_depth=7, max_leaves=0, min_child_weight=5,\n",
    "              missing=np.nan, monotone_constraints='()', n_estimators=200,\n",
    "              n_jobs=-1, nthread=-1, num_parallel_tree=1, predictor='auto',\n",
    "              random_state=0, reg_alpha=0)\n",
    "train_model(xgb, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72206c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    # A parameter grid for XGBoost\n",
    "    params = {\n",
    "            'min_child_weight': [1, 5, 10, 15, 20, 25, 30],\n",
    "            'gamma': [0.1, 0.5, 1, 1.5, 2, 5, 10],\n",
    "            'subsample': [0.4, 0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n",
    "            'max_depth': np.arange(1, 10, 2).tolist(),\n",
    "            'n_estimators': np.arange(0, 500, 50).tolist(),\n",
    "            'learning_rate': [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 10],\n",
    "            }\n",
    "\n",
    "    xgb = XGBClassifier(objective='binary:logistic', nthread=-1)\n",
    "    #random_search = GridSearchCV(xgb, param_grid=params, scoring='accuracy', n_jobs=-1, cv=cv, verbose=1)\n",
    "    random_search = RandomizedSearchCV(xgb, params, scoring='accuracy', n_jobs=-1, cv=cv, n_iter=200, verbose=1)\n",
    "\n",
    "    # Here we go\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed0442",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "train_model(ada, 'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_hyperparameter_tuning:\n",
    "    params = {\n",
    "        \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 250, 500],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 10],\n",
    "    }\n",
    "\n",
    "    search_cv = GridSearchCV(AdaBoostClassifier(), param_grid=params, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    search_cv.fit(X_train, y_train)\n",
    "    print(search_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d7bf9",
   "metadata": {},
   "source": [
    "### LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10570369",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.2,\n",
    "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
    "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "               n_estimators=400, n_jobs=-1, num_leaves=31, objective=None,\n",
    "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, verbose=-1,\n",
    "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "train_model(lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b82b36",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03524711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CatBoostClassifier(verbose=False, depth=6, learning_rate=0.1, rsm=0.5, l2_leaf_reg=10, min_data_in_leaf=20,\n",
    "                       random_strength=0.175)\n",
    "train_model(cb, 'CatBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e1761",
   "metadata": {},
   "source": [
    "### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = []\n",
    "estimator.append(('CB', cb))\n",
    "estimator.append(('LGB', lgb))\n",
    "#estimator.append(('RF', rf))\n",
    "#estimator.append(('GB', gb))\n",
    "estimator.append(('XGB', xgb))\n",
    "#estimator.append(('ADA', ada))\n",
    "#estimator.append(('PA', pa))\n",
    "estimator.append(('KNN', knn))\n",
    "estimator.append(('SVM', svm))\n",
    "#estimator.append(('LinSVM', linsvm))\n",
    "#estimator.append(('DT', dt))\n",
    "estimator.append(('MLP', mlp))\n",
    "#estimator.append(('NB', nb))\n",
    "estimator.append(('LR', lr))\n",
    "\n",
    "hv = VotingClassifier(estimators = estimator, voting ='hard')\n",
    "train_model(hv, 'Hard Voting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ca758",
   "metadata": {},
   "source": [
    "### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "train_model(sv, 'Soft Voting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4b347",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_s = []\n",
    "#estimator_s.append(('CB', cb))\n",
    "#estimator_s.append(('LGB', lgb))\n",
    "#estimator_s.append(('RF', rf))\n",
    "#estimator_s.append(('GB', gb))\n",
    "#estimator_s.append(('XGB', xgb))\n",
    "#estimator_s.append(('ADA', ada))\n",
    "#estimator_s.append(('PA', pa))\n",
    "estimator_s.append(('KNN', knn))\n",
    "#estimator_s.append(('SVM', svm))\n",
    "#estimator_s.append(('LinSVM', linsvm))\n",
    "estimator_s.append(('DT', dt))\n",
    "#estimator_s.append(('MLP', mlp))\n",
    "estimator_s.append(('NB', nb))\n",
    "estimator_s.append(('LR', lr))\n",
    "\n",
    "st = StackingClassifier(estimators=estimator_s,\n",
    "                        final_estimator=lr,\n",
    "                        cv=cv,\n",
    "                        n_jobs=-1,\n",
    "                        passthrough=True,\n",
    "                        verbose=1)\n",
    "\n",
    "train_model(st, 'Stacking')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "628af5e6",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1903f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7b50d6",
   "metadata": {},
   "source": [
    "# Fake Reviews - Data Cleaning\n",
    "\n",
    "---\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re, contractions, string, unicodedata\n",
    "from num2words import num2words\n",
    "\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.spatial.distance import cdist\n",
    "import networkx as nx\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd39d58",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87569ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('deceptive.csv')\n",
    "\n",
    "# lowercase columns\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# select target class and text column\n",
    "df = df[['text', 'class']]\n",
    "\n",
    "# show two rows\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4bcce8f",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Cleaning\n",
    "\n",
    "- [Source for Chat Expressions](https://raw.githubusercontent.com/MFuchs1989/Datasets-and-Miscellaneous/main/datasets/NLP/Text%20Pre-Processing%20VII%20(Special%20Cases)/chat_expressions.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54918a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nltk & spacy stop words\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "spacy_stopwords = set(spacy.load('en_core_web_sm').Defaults.stop_words)\n",
    "\n",
    "# union between two sets of stop words\n",
    "stopwords = nltk_stopwords.union(spacy_stopwords)\n",
    "\n",
    "# dictionary of short forms to original forms\n",
    "chat_expressions = pd.read_csv('chat_expressions.csv', on_bad_lines='error')\n",
    "chat_expressions_dict = dict(zip(chat_expressions.Chat_Words, chat_expressions.Chat_Words_Extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos_func(word):\n",
    "    '''\n",
    "    Maps the respective POS tag of a word to the format accepted by the lemmatizer of wordnet\n",
    "    \n",
    "    Args:\n",
    "        word (str): Word to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        POS tag, readable for the lemmatizer of wordnet\n",
    "    '''     \n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cba18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    text: str,    \n",
    "    normalization: List[str]=[],\n",
    "    stop: str='',\n",
    "    expand_contractions=False,\n",
    "    numbers_to_words=False,\n",
    "    expand_short_forms=False,\n",
    "    remove_special_characters=False,\n",
    "    remove_punctuation=False,\n",
    "    seperate_numbers_from_text=False,\n",
    "    convert_numbers_to_text=False,\n",
    "    only_alphabets=False,\n",
    "    only_alphabets_numbers=False, \n",
    "    custom_words=[]\n",
    "    )-> str:\n",
    "    '''\n",
    "    Goes through a series of preprocessing steps to clean the supplied text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be cleaned\n",
    "        normalization (List[str], optional): List of normalization functions to apply\n",
    "                                             [stem|lem|lem_v|lem_a|lem_pos|spacy]. Defaults to [].\n",
    "        stop (str, optional): Choice of stop words dictionary to use [nltk|spacy|both|custom]. Defaults to ''.\n",
    "        expand_contractions (bool, optional): Expand contractions. Defaults to False.\n",
    "        numbers_to_words (bool, optional): Convert numbers to words. Defaults to False.\n",
    "        expand_short_forms (bool, optional): Expand short forms. Defaults to False.\n",
    "        remove_special_characters (bool, optional): Remove special characters. Defaults to False.\n",
    "        remove_punctuation (bool, optional): Remove punctuation. Defaults to False.\n",
    "        seperate_numbers_from_text (bool, optional): Seperate numbers from text. Defaults to False.\n",
    "        convert_numbers_to_text (bool, optional): Convert numbers to text. Defaults to False.\n",
    "        only_alphabets (bool, optional): Remove all characters except alphabets. Defaults to False.\n",
    "        only_alphabets_numbers (bool, optional): Remove all characters except alphabets and numbers. Defaults to False.\n",
    "        custom_words (list, optional): List of custom words to be added to the stop words dictionary. Defaults to [].\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    '''\n",
    "    \n",
    "    # remove emails\n",
    "    text = ' '.join([i for i in text.split() if '@' not in i])\n",
    "    \n",
    "    # remove web address\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    \n",
    "    # remove line breaks\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    \n",
    "    # removing accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # expand contractions\n",
    "    if expand_contractions:\n",
    "        text = contractions.fix(text)\n",
    "    \n",
    "    #short forms to normal forms\n",
    "    if expand_short_forms:\n",
    "        text = re.sub(r'\\S+', lambda m: chat_expressions_dict.get(m.group().upper(), m.group()) , text)\n",
    "    \n",
    "    # make text lower\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove brackets and parantheses (and removing the text inside the brackets and parantheses)\n",
    "    if remove_punctuation:\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        \n",
    "    # remove multiple punctuations\n",
    "    text = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "    \n",
    "    # separate numbers from text\n",
    "    if seperate_numbers_from_text:\n",
    "        text = re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", text).strip() \n",
    "    \n",
    "    # convert numbers to textual representation\n",
    "    if convert_numbers_to_text:\n",
    "        after_spliting = text.split()\n",
    "        for index in range(len(after_spliting)):\n",
    "            if after_spliting[index].isdigit():\n",
    "                after_spliting[index] = num2words(after_spliting[index])\n",
    "        text = ' '.join(after_spliting)\n",
    "    \n",
    "    # filter to allow only alphabets\n",
    "    if only_alphabets:\n",
    "        text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    # filter to allow numbers and alphabets\n",
    "    elif only_alphabets_numbers:   \n",
    "        text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text)\n",
    "    \n",
    "    # remove unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    #remove double spaces \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # stop words [options: nltk, spacy, both, custom]\n",
    "    if stop == 'nltk':\n",
    "        text = text.split()\n",
    "        text = [word for word in text if not word in nltk_stopwords]\n",
    "    elif stop == 'spacy':\n",
    "        text = text.split()\n",
    "        text = [word for word in text if not word in spacy_stopwords]\n",
    "    elif stop == 'both':\n",
    "        text = text.split()\n",
    "        text = [word for word in text if not word in stopwords]\n",
    "    elif stop == 'custom':\n",
    "        text = text.split()\n",
    "        stopwords_list = list(stopwords)\n",
    "        stopwords_list.extend(custom_words)\n",
    "        text = [word for word in text if not word in stopwords_list]\n",
    "    \n",
    "    # stemming & lemmatization\n",
    "    for setting in normalization:\n",
    "        if setting == 'stem':\n",
    "            stemmer = PorterStemmer() \n",
    "            text = [stemmer.stem(y) for y in text]\n",
    "        if setting == 'lan':\n",
    "            stemmer = LancasterStemmer() \n",
    "            text = [stemmer.stem(y) for y in text]\n",
    "        if setting == 'snow':\n",
    "            stemmer = EnglishStemmer() \n",
    "            text = [stemmer.stem(y) for y in text]\n",
    "        if setting == 'lem':\n",
    "            lem = WordNetLemmatizer()\n",
    "            text = [lem.lemmatize(y) for y in text]\n",
    "        if setting == 'lem_v':\n",
    "            lem = WordNetLemmatizer()\n",
    "            text = [lem.lemmatize(y, pos='v') for y in text]\n",
    "        if setting == 'lem_a':\n",
    "            lem = WordNetLemmatizer()\n",
    "            text = [lem.lemmatize(y, pos='a') for y in text]\n",
    "        if setting == 'lem_pos':\n",
    "            lem = WordNetLemmatizer()\n",
    "            text = [lem.lemmatize(y, get_wordnet_pos_func(y)) for y in text]\n",
    "        if setting == 'spacy':\n",
    "            text = nlp(' '.join(text))\n",
    "            text = [y.lemma_ for y in text]\n",
    "        \n",
    "    if stop != '':\n",
    "        return ' '.join(text)\n",
    "    else:\n",
    "        return ''.join(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e0a4bd9",
   "metadata": {},
   "source": [
    "### Run Data Cleaning\n",
    "\n",
    "- We create a series of various pre-processed text to assess which pre-processing steps provide the best performance.\n",
    "- Please refer the the parameters inside the `clean_text` function to see which pre-processing steps are applied.\n",
    "- Certain pre-processing steps inside the `clean_text` function are always applied, those are the ones without `if-else` statements.\n",
    "- `text` column is the original review, `tN` where `N` is the code to distinguish between different pre-processed texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac9c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "df['t01'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, only_alphabets=True))\n",
    "df[['text', 't01', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t02'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=False, only_alphabets=False))\n",
    "df[['text', 't02', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d551e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t03'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True))\n",
    "df[['text', 't03', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27751862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t04'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, stop='nltk'))\n",
    "df[['text', 't04', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87654038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t05'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, stop='spacy'))\n",
    "df[['text', 't05', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t06'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, stop='both'))\n",
    "df[['text', 't06', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799649ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t07'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom', \n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't07', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86080ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t08'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['stem'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't08', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a04552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t09'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['lem'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't09', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t10'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['lem_pos'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't10', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bdd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t11'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['spacy'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't11', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t12'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['lem', 'stem'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't12', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712d562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['t13'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['lem_pos', 'stem'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't13', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t14'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['lan'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't14', 'class']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b408cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['t15'] = df['text'].apply(lambda x: clean_text(x, expand_contractions=True,\n",
    "                                                  expand_short_forms=True,\n",
    "                                                  remove_punctuation=True, \n",
    "                                                  only_alphabets=True, \n",
    "                                                  seperate_numbers_from_text=True, \n",
    "                                                  convert_numbers_to_text=True, \n",
    "                                                  stop='custom',\n",
    "                                                  normalization=['snow'],\n",
    "                                                  custom_words=['hotel', 'room', 'chicago', 'great', 'stay',\n",
    "                                                                'hard', 'rock', 'fairmont', 'millennium', \n",
    "                                                                'sheraton', 'towers', 'sofitel', 'knickerbocker', \n",
    "                                                                'water']))\n",
    "df[['text', 't15', 'class']].head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ca588f",
   "metadata": {},
   "source": [
    "### Save Pre-Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('deceptive-cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9b954",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e680587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cleaned dataset\n",
    "df = pd.read_csv(\"deceptive-cleaned.csv\")\n",
    "\n",
    "# drop duplicates and keep the last one\n",
    "df = df.drop_duplicates(subset=['t10'], keep='last')\n",
    "\n",
    "# drop rows with empty text\n",
    "df = df[~df['t10'].str.contains(r'^\\s*$', na=False)]\n",
    "\n",
    "# reset the index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset: X = text, y = class\n",
    "X = df['t10']\n",
    "y = df['class']\n",
    "\n",
    "# generate the tf-idf matrix\n",
    "tfid_vec = TfidfVectorizer()\n",
    "corpus = tfid_vec.fit_transform(X)\n",
    "feature_names = tfid_vec.get_feature_names_out()\n",
    "corpus_arr = pd.DataFrame(corpus.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dimensionality of the tf-idf matrix\n",
    "tsne = TSNE(n_components=2, init='pca', learning_rate='auto', n_jobs=-1)\n",
    "tfidf_2d = tsne.fit_transform(corpus_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0eaa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw the points\n",
    "alpha = 0.5\n",
    "label_map = {l: i for i, l in enumerate(np.unique(df['class']))}\n",
    "\n",
    "# map colours to the target labels\n",
    "node_colours = [label_map[target] for target in df['class']]\n",
    "\n",
    "# generate the plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    tfidf_2d[:, 0],\n",
    "    tfidf_2d[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16627c6f",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e2e25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_best_features(X, y, k = 2000, convert_to_frame=False):\n",
    "    '''\n",
    "    Selects the best features using the chi-squared test\n",
    "    \n",
    "    Args:\n",
    "        k (int, optional): Number of features to select. Defaults to 2000.\n",
    "        \n",
    "    Returns:\n",
    "        Array of selected features\n",
    "    '''\n",
    "    \n",
    "    chi2_features = SelectKBest(chi2, k=k)\n",
    "    X_new = chi2_features.fit_transform(X, y)\n",
    "    \n",
    "    if convert_to_frame:\n",
    "        chi_support = chi2_features.get_support()\n",
    "        chi_feature = X.loc[:, chi_support].columns.tolist()\n",
    "        X_new = pd.DataFrame(X_new, columns=chi_feature)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# select the best features using chi-squared test (default k=2000)\n",
    "X_new = select_best_features(corpus_arr, y, convert_to_frame=True)\n",
    "print(f'Before feature selection: {corpus_arr.shape}\\nAfter feature selection: {X_new.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c2317",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Distance measures tested:**\n",
    "- Euclidean distance\n",
    "- Cosine similarity\n",
    "- Hamming distance\n",
    "- Chebyshev distance\n",
    "- Jaccard distance\n",
    "- Cityblock distance\n",
    "- Minkowski distance\n",
    "- Canberra distance\n",
    "- Braycurtis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06627a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the distance metric to use\n",
    "# generate the adjacency matrix for distance metrics\n",
    "\n",
    "adj_X = cdist(X_new, X_new, 'hamming')\n",
    "#adj_X = cdist(X_new, X_new, 'cosine')\n",
    "#adj_X = cdist(X_new, X_new, 'chebyshev')\n",
    "#adj_X = cdist(X_new, X_new, 'jaccard')\n",
    "#adj_X = cdist(X_new, X_new, 'cityblock')\n",
    "#adj_X = cdist(X_new, X_new, 'minkowski')\n",
    "#adj_X = cdist(X_new, X_new, 'canberra')\n",
    "#adj_X = cdist(X_new, X_new, 'braycurtis')\n",
    "#adj_X = cdist(X_new, X_new, 'euclidean')\n",
    "\n",
    "# calculate the average of the adjacency matrix\n",
    "avg_adj_X = np.average(adj_X)\n",
    "\n",
    "# convert the adjacency matrix to binary\n",
    "adj_X = np.where(adj_X >= avg_adj_X, 1, 0)\n",
    "\n",
    "# convert the adjacency matrix type to int8 to reduce memory usage\n",
    "adj_X = adj_X.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b429a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the networkx graph from the binarized adjacency matrix\n",
    "X_netwrk = nx.from_numpy_matrix(adj_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93869081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the graph from the networkx graph\n",
    "X_graph = StellarGraph.from_networkx(X_netwrk)\n",
    "print(X_graph.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BiasedRandomWalk object\n",
    "brw = BiasedRandomWalk(X_graph)\n",
    "\n",
    "# run BiasedRandomWalk on the graph nodes and print the number of random walks\n",
    "walks = brw.run(\n",
    "    nodes=list(X_graph.nodes()),  # root nodes\n",
    "    length=100,  # maximum length of a random walk\n",
    "    n=10,  # number of random walks per root node\n",
    "    p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=1.0,  # Defines (unormalised) probability, 1/q, for moving away from source node \n",
    "            # -> check with the t-SNE figure below to see if the nodes are well separated, otherwise adjust the p & q values accordingly\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the random walks to strings\n",
    "str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "\n",
    "# train the Word2Vec model on the random walks\n",
    "model = Word2Vec(str_walks, window=5, min_count=0, sg=1, workers=5, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the node embedding indexes\n",
    "node_ids = model.wv.index_to_key\n",
    "\n",
    "# get the node embeddings\n",
    "node_embeddings = (\n",
    "    model.wv.vectors\n",
    ")  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "\n",
    "# get the node target classes\n",
    "node_targets = df['class'][[int(node_id) for node_id in node_ids]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2, init='pca', learning_rate='auto', n_jobs=-1)\n",
    "node_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "# draw the points\n",
    "alpha = 0.7\n",
    "label_map = {l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [label_map[target] for target in df['class']]\n",
    "\n",
    "# generate the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    node_embeddings_2d[:, 0],\n",
    "    node_embeddings_2d[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of node embeddings and node target classes\n",
    "node_embeddings_df = pd.DataFrame(node_embeddings)\n",
    "node_targets_df = pd.DataFrame(node_targets)\n",
    "node_targets_df.sort_index()\n",
    "\n",
    "node_embeddings_df['class'] = node_targets_df\n",
    "    \n",
    "# save as pickle to reserve the data as it is\n",
    "node_embeddings_df.to_pickle('t10.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7fdd9a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Basic Modeling to Test after Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e582a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = node_embeddings_df.drop(['class'], axis=1)\n",
    "y = node_embeddings_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(max_iter=10000) \n",
    "log.fit(x_train, y_train)\n",
    "y_pred=log.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('accuracy_score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662353b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm = LinearSVC()\n",
    "lsvm.fit(x_train, y_train)\n",
    "y_pred=lsvm.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('accuracy_score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53281fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
